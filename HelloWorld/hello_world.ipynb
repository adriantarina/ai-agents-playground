{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28ffa354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "160911ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d80e964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef7b9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    message: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38aef608",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed00f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemini-2.5-flash\"\n",
    "llm = ChatOpenAI(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90c38eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x21277c5e150>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def say_hello(old_state: State) -> State:\n",
    "\n",
    "    response = llm.invoke(old_state[\"message\"])\n",
    "    return State(message=response.choices[0].message.content)\n",
    "\n",
    "graph_builder.add_node(\"first_node\", say_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8102dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x21277c5e150>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(START, \"first_node\")\n",
    "graph_builder.add_edge(\"first_node\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd4f26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c285b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(user_input: str, history):\n",
    "    state = State(message=user_input)\n",
    "    result = graph.invoke(state)\n",
    "    print(result)\n",
    "    return result[\"message\"]\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b100aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2125, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1605, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1033, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 541, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 902, in _submit_fn\n",
      "    response = await run_sync(self.fn, *inputs, limiter=self.limiter)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_33996\\2793747654.py\", line 3, in chat\n",
      "    result = graph.invoke(state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\work\\workspace\\ai-agents-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_33996\\1172194423.py\", line 3, in say_hello\n",
      "    response = llm.invoke(old_state[\"message\"])\n",
      "                          ~~~~~~~~~^^^^^^^^^^^\n",
      "TypeError: 'State' object is not subscriptable\n",
      "During task with name 'first_node' and id 'fd60b822-7105-688a-c0f3-13446bfca35c'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
